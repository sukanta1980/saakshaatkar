------------------------------------------------------------------------------------------------------------------------------------------------------------------
Kubernetes Architecture
------------------------------------------------------------------------------------------------------------------------------------------------------------------

Kubernetes follows a master-slave architecture where the master node controls multiple worker nodes that run containerized applications.

1. Master Node:
   - API Server: Frontend for Kubernetes, exposes the Kubernetes API.
   - Scheduler: Assigns nodes for newly created pods.
   - Controller Manager: Monitors cluster state and reconciles desired state.
   - etcd: Consistent and highly available key-value store for cluster data.

2. Worker Nodes:
   - Kubelet: Agent that runs on each node and manages the node and its containers.
   - Kube Proxy: Network proxy that reflects services as defined in the Kubernetes API.
   - Container Runtime: Software responsible for running containers (e.g., Docker, containerd).

3. Pods:
   - Smallest deployable units in Kubernetes, consisting of one or more containers.


                                +--------------------------+
                                |        API Server         |
                                +--------------------------+
                                         |
                                         v
                                +--------------------------+
                                |      Scheduler           |
                                +--------------------------+
                                         |
                                         v
                               +--------------------------+
                               | Controller Manager       |
                               +--------------------------+
                                         |
                                         v
                                    etcd (KV store)
                                         |
                                         v
                                  +-----------+
                                  | Master    |
                                  | Node      |
                                  +-----------+
                                  /          \
                                 /            \
                                v              v
                      +--------------+   +--------------+
                      | Worker Node  |   | Worker Node  |
                      | (Kubelet)    |   | (Kubelet)    |
                      +--------------+   +--------------+
                      |  Container   |   |  Container   |
                      |  Runtime     |   |  Runtime     |
                      +--------------+   +--------------+

------------------------------------------------------------------------------------------------------------------------------------------------------------------
Pod : The smallest and simplest Kubernetes object. A pod represents a single instance of a running process in the cluster and can contain one or more containers that share resources.
Node : A worker machine in Kubernetes, which can be either a virtual or physical machine. Each node runs pods and is managed by the control plane.
Cluster :A set of nodes grouped together, managed by a control plane, which orchestrates the deployment, scaling, and management of containerized applications.
Namespace:logical partition within a Kubernetes cluster that provides a way to divide cluster resources between multiple users or teams, allowing for resource isolation.
Deployment:controller that manages stateless applications by ensuring that a specified number of pod replicas are running at any given time. Supports rolling updates and rollbacks.
ReplicaSet: Ensures that a specified number of pod replicas are running at all times. It is primarily used by Deployments to maintain the desired state.
StatefulSet:Manages stateful applications, providing stable, unique network identifiers, and persistent storage. Ensures ordered and graceful deployment and scaling.
DaemonSet:Ensures that a copy of a pod runs on all (or some) nodes in the cluster, often used for background tasks like logging or monitoring.
Job:Creates one or more pods and ensures that a specified number of them successfully terminate. Used for batch or short-lived tasks.
CronJob:Manages time-based jobs, running on a scheduled basis, similar to cron in Unix/Linux systems.
Service: You have a set of Pods running a web application, and you want to expose these Pods internally within the cluster or to the external world. You would create a Service to provide a stable endpoint and load balancing.
Service Account: You have a Pod that needs to interact with the Kubernetes API to list or create resources. You would create a service account with the necessary permissions and associate it with the Pod.ConfigMap: Stores non-sensitive configuration data as key-value pairs. ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable.
Secret: Stores sensitive data such as passwords, OAuth tokens, and SSH keys. Secrets are similar to ConfigMaps but are intended to hold sensitive information.
PersistentVolume (PV): A piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using StorageClasses. PVs are a cluster resource.
PersistentVolumeClaim (PVC): request for storage by a user. PVCs consume PersistentVolumes in the cluster and can request specific size and access modes.
StorageClass: Provides a way to describe the "classes" of storage that can be offered in a cluster. Different classes might map to quality-of-service levels, backup policies, or arbitrary policies determined by the cluster administrators.
Kubelet: An agent that runs on each node in the cluster, ensuring that containers are running in a pod as specified. It communicates with the control plane components.
Kube-proxy: Maintains network rules on nodes. These network rules allow network communication to pods from network sessions inside or outside of the cluster.
etcd: distributed key-value store used to store all cluster data. It is the backend for service discovery and stores the cluster's state and configuration.
Control Plane: The collection of components that manage the state of the cluster, including the API server, scheduler, controller manager, and etcd.
API Server (kube-apiserver): The component that exposes the Kubernetes API. It is the front-end for the Kubernetes control plane.
Scheduler (kube-scheduler): Watches for newly created pods that do not have an assigned node and selects a node for them to run on based on resource availability and other constraints.
Operator: A method of packaging, deploying, and managing a Kubernetes application. Operators use custom resources to manage applications and their components.
Replica: The term "replica" refers to the number of identical copies of a pod that a Kubernetes deployment ensures are running at any given time.
Resource Quota: Limits the amount of resources (CPU, memory, storage) that a namespace can consume. It helps ensure fair resource distribution among multiple namespaces.
LimitRange: Defines default resource limits and requests for pods and containers within a namespace, helping to manage resource usage.
Affinity and Anti-Affinity: Rules that influence pod placement on nodes. Affinity specifies preferences for co-locating pods, while anti-affinity specifies preferences for separating pods across nodes.
Taints and Tolerations: Mechanisms to control pod scheduling. Taints are applied to nodes to repel pods, while tolerations are applied to pods to allow them to schedule on nodes with matching taints.
Horizontal Pod Autoscaler (HPA): Automatically adjusts the number of pod replicas in a deployment, replication controller, or stateful set based on observed CPU utilization or other select metrics.
Vertical Pod Autoscaler (VPA): Automatically adjusts the resource limits and requests (CPU and memory) of pods to match the resource needs of the application.
PodDisruptionBudget (PDB): Specifies the minimum number of replicas of a pod that must be up at any given time during voluntary disruptions (e.g., updates, deletions).
NetworkPolicy: Defines rules for how pods communicate with each other and with other network endpoints. It controls traffic flow at the IP address or port level.
ConfigMap: A Kubernetes object used to store non-sensitive configuration data as key-value pairs. ConfigMaps decouple configuration artifacts from container images to keep containerized applications portable.
Secret: A Kubernetes object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys, securely.
Ingress: Kubernetes resource that manages external access to services within the cluster via HTTP/HTTPS routes.
Ingress Controller: Manages and configures Ingress rules, typically using a reverse proxy (e.g., Nginx, Traefik) to route traffic to services based on defined rules and policies.
Init Container:Containers that run before the app containers in a pod are started. They are used to set up prerequisites for the application containers.
Sidecar Container:A helper container that runs alongside the main container in a pod and provides additional functionality, such as logging or proxying.
PodPreset:Kubernetes resource used to inject configuration data into pods at creation time, such as environment variables, volumes, and volume mounts.
------------------------------------------------------------------------------------------------------------------------------------------------------------------

Init Containers and Sidecar Containers in Kubernetes
------------------------------------------------------------------------------------------------------------------------------------------------------------------
Init Containers are specialized containers that run before app containers in a pod. They are used to perform initialization tasks such as setting up configurations, checking prerequisites, or initializing data. Init containers run to completion before the main application containers start.
Consider a scenario where a main application needs some configuration files to be present before it starts. An init container can be used to download these files.

apiVersion: v1
kind: Pod
metadata:
  name: init-container-example
spec:
  containers:
  - name: main-app
    image: my-app:latest
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  initContainers:
  - name: init-config
    image: busybox:latest
    command: ['sh', '-c', 'wget -O /etc/config/config.yaml http://example.com/config.yaml']
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config
  volumes:
  - name: config-volume
    emptyDir: {}

Sidecar Containers run alongside the main application container within the same pod. They are used to extend and enhance the functionality of the main application. Common use cases include logging, monitoring, or proxying.
Consider a scenario where the main application writes logs to a local file, and a sidecar container is used to send these logs to a remote logging service.

apiVersion: v1
kind: Pod
metadata:
  name: sidecar-container-example
spec:
  containers:
  - name: main-app
    image: my-app:latest
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app
    command: ['sh', '-c', 'echo "Starting app"; while true; do echo $(date) >> /var/log/app/log.txt; sleep 5; done']
  - name: log-forwarder
    image: busybox:latest
    volumeMounts:
    - name: log-volume
      mountPath: /var/log/app
    command: ['sh', '-c', 'tail -F /var/log/app/log.txt | nc logging-service.example.com 1234']
  volumes:
  - name: log-volume
    emptyDir: {}

------------------------------------------------------------------------------------------------------------------------------------------------------------------
What is Control Plane in kubernetes
------------------------------------------------------------------------------------------------------------------------------------------------------------------

In Kubernetes, the control plane is the brain and central nervous system of the entire orchestration platform. It's responsible for making high-level decisions about the cluster, issuing commands, and ensuring the desired state of your containerized applications is maintained. Here's a breakdown of the control plane's key functionalities:

Cluster Management: The control plane provisions and manages the Kubernetes cluster itself. This includes adding or removing nodes, configuring security settings, and overseeing the overall health of the cluster.
Scheduling: When you deploy a new application (represented as pods in Kubernetes), the control plane's scheduler component takes center stage. It analyzes available resources across the cluster nodes and assigns pods to suitable nodes based on factors like resource requirements (CPU, memory), pod priorities, and any specified affinity/anti-affinity rules (e.g., placing related pods together or keeping them separate).
Self-Healing: The control plane constantly monitors the health of your pods. If a pod becomes unhealthy or crashes, the control plane's replica set controller detects the issue and takes corrective actions. This might involve restarting the pod on the same node or scheduling a new pod on a different healthy node, ensuring your applications remain available even in case of failures.
Service Management: The control plane plays a crucial role in exposing your containerized applications as services. It creates Kubernetes services that act as abstractions for pods, providing a stable network identity and load balancing capabilities. This allows external users or other applications within the cluster to access your services without needing to know the underlying pod details or their network addresses.
Security: The control plane enforces security policies within the cluster. This might involve authentication and authorization mechanisms to control access to the cluster and its resources. Additionally, security policies can be defined to restrict pod behavior or network access for enhanced security.

Components of the Control Plane:

Kubernetes API Server: This is the central communication hub for the control plane. It exposes a RESTful API that allows users (through kubectl or other tools) and other Kubernetes components to interact with the cluster. Deployments are submitted, pod information is retrieved, and cluster configurations are managed through the API server.
Kube-scheduler: As mentioned earlier, the scheduler is responsible for making informed decisions about where to place pods across the available nodes in the cluster.
Kube-controller-manager: This component runs a collection of controllers that constantly work in the background to ensure the cluster state matches the desired state as specified in your deployments and service configurations. Replica set controllers, service controllers, and namespace controllers are some examples.
etcd (Optional): While not strictly mandatory, etcd is a popular choice for storing cluster state information in a distributed key-value store. This ensures all components of the control plane have a consistent view of the cluster's current state and desired state.


------------------------------------------------------------------------------------------------------------------------------------------------------------------
if a kubernetes pod is going to crash loopback state ,  What are the steps to debug and fix it 
------------------------------------------------------------------------------------------------------------------------------------------------------------------

Identify the Pod: kubectl get pods -n -o wide
Check the Pod Description:
Check the Events section
Analyze Container Logs:
Check Resource Limits:
Verify Application Compatibility:
Review Image Health:
Debug Liveness and Readiness Probes (if defined):
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is PDB
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In Kubernetes, PDB stands for Pod Disruption Budget. It is a policy object that defines the minimum number or percentage of pods that must remain running during voluntary disruptions. Voluntary disruptions are those that are planned by administrators, such as upgrading a node or draining a node for maintenance. PDBs help ensure the availability and reliability of applications during these disruptions by preventing too many pods from being simultaneously taken down.

Key Concepts of PDB
Voluntary Disruptions: These are planned actions initiated by users or cluster administrators, such as node upgrades or maintenance.
Involuntary Disruptions: These are unexpected and unplanned disruptions, such as node failures or hardware issues. PDBs do not protect against these.
MinAvailable: Specifies the minimum number of pods that must be available.
MaxUnavailable: Specifies the maximum number of pods that can be unavailable.

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-app-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: my-app

In this example:

apiVersion: Specifies the API version (policy/v1).
kind: Specifies the type of Kubernetes object (PodDisruptionBudget).
metadata: Contains the name and labels for the PDB.
spec: Defines the PDB's behavior.
minAvailable: Ensures that at least one pod matching the selector is always available.
selector: Identifies the pods to which this PDB applies using label selectors.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In Kubernetes, a Service Account is a special type of account used by pods to authenticate to the Kubernetes API. It provides:

Identity: For pods to interact with the Kubernetes API.
Access Control: Allows fine-grained access management using Role-Based Access Control (RBAC).
Isolation: Each pod can have its own service account, enhancing security.
Tokens: Automatically mounted into pods for authentication.

Service accounts ensure secure and controlled access to the Kubernetes API from within the cluster.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is RBAC

RBAC stands for Role-Based Access Control. 
RBAC in Kubernetes provides a robust framework for managing access control, ensuring security and efficient resource allocation for different teams within your cluster

We have a development, staging, and production environment in our Kubernetes cluster.
We have three teams: Dev, QA, and Ops.
We want to restrict access to resources based on team responsibilities.

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups:
  - ""
  resources:
  - deployments
  - pods
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - delete
namespace: dev

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Ingress Vs Ingress Controller
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

An Ingress resource is a YAML or JSON configuration object defined within Kubernetes.
It acts as a high-level way to describe how external traffic should be routed to services in your cluster.
An Ingress typically specifies:
Hostname(s): The domain name(s) that should route traffic to the cluster.
Paths: How to map incoming URLs to specific services or backend applications.
Load balancing: How to distribute traffic across multiple instances of a service (optional).
TLS termination: Whether to terminate SSL/TLS encryption at the Ingress or pass it to backend services (optional).

An Ingress controller is a separate software component that runs inside your Kubernetes cluster.
It acts as a mediator between the Ingress resource and the underlying Kubernetes services.
The Ingress controller watches for Ingress resources to be created or updated.
It translates the configuration defined in the Ingress resource into specific rules for your load balancer, reverse proxy, or other edge infrastructure.
The Ingress controller is responsible for:
Configuring the load balancer or reverse proxy to route traffic based on the Ingress rules.
Handling features like SSL termination if specified in the Ingress resource.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What are the different ways to provide external network connectivity to K8?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Load Balancers: Use cloud provider load balancers (e.g., AWS ELB, GCP LB) to expose services.
NodePort: Expose a service on a static port on each node's IP.
Ingress: Manage external access using HTTP/HTTPS routes with an Ingress controller (e.g., Nginx, Traefik).

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
What is the difference between ConfigMap Vs Secretes 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ConfigMap: Stores non-sensitive configuration data
Secret: Stores sensitive data such as passwords, tokens, and keys.
ConfigMap: Data is stored as plain text.
Secret: Data is base64 encoded.
ConfigMap: Less secure, as data is not encrypted.
Secret: More secure, with stricter access controls and encryption at rest.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Please explain kubernetes PVs and PVC with an example
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Persistent Volume (PV): A piece of storage in the cluster provisioned by an administrator.
Persistent Volume Claim (PVC): A request by a user for storage.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Explain how Kubernetes ensures fault tolerance. What mechanisms are in place for disaster recovery?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Replication: Deployments and ReplicaSets ensure multiple copies of pods, distributing them across nodes.
Node Health Checks: Kubernetes regularly checks node health and replaces failed nodes.
Self-Healing: Automatically restarts failed containers and reschedules pods on healthy nodes.
Pod Disruption Budgets: Ensures a minimum number of pods are always available during maintenance or updates.
Persistent Storage: Use of Persistent Volumes (PVs) for data durability across pod restarts and node failures.
Backups: Regular backups of etcd, the cluster's key-value store, for disaster recovery.
Multi-AZ Deployments: Deploying nodes across multiple Availability Zones (AZs) to mitigate zone failures.
Disaster Recovery Plans: Automated scripts and tools for cluster restoration from backups.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Node Affinity:
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Purpose: Controls where Pods are scheduled to run within the Kubernetes cluster.
How it Works: You define rules (e.g., requiring a specific label or hardware characteristic) in the Pod spec. The Kubernetes scheduler only places the Pod on nodes that meet these requirements.
Use Cases:
Ensure Pods requiring specific resources (e.g., GPU) run on nodes with those capabilities.
Co-locate Pods from the same application on the same node for better performance.
Isolate critical Pods from non-critical ones for improved resource management.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Pod Disruption Budget (PDB):
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Purpose: Sets limits on how many Pods in a deployment or StatefulSet can be unavailable due to voluntary disruptions (e.g., node drain for maintenance).
How it Works: You define a minimum number of Pods (absolute value) or a percentage of replicas that must be available at any given time. Kubernetes ensures this threshold is met during voluntary disruptions by selectively evicting Pods.
Use Cases:
Guarantee a minimum level of service availability during planned maintenance or upgrades.
Prevent service outages caused by accidental deletion or disruption of Pods.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Horizontal Pod Autoscaler (HPA):
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Purpose: Automatically scales the number of Pods in a deployment based on predefined metrics like CPU or memory usage.
How it Works: You define the desired metrics and scaling thresholds. When the metrics reach a defined limit, the HPA scales the deployment up (adds replicas) or down (removes replicas) to maintain optimal resource utilization.
Use Cases:
Automatically scale applications to meet fluctuating traffic demands.
Optimize resource utilization by avoiding under-provisioning or over-provisioning of Pods.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is etcd?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

etcd is a distributed key-value store used to store configuration data and state information in a highly available and consistent manner. It is a critical component of Kubernetes, where it stores all cluster data.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Why is etcd important for Kubernetes?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

etcd stores all the configuration data, state information, and metadata about the Kubernetes cluster. This includes information about nodes, pods, services, secrets, and more. It ensures the cluster's state is consistent and highly available.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

How do you deploy etcd in a Kubernetes cluster?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

etcd can be deployed as a standalone cluster or as part of the Kubernetes control plane. Typically, it's deployed using a StatefulSet in Kubernetes to ensure stable network identities and persistent storage. An example configuration might include specifying replicas, storage volumes, and secure communication.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

How do you ensure high availability for etcd?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

High availability is achieved by running etcd in a multi-node cluster (typically 3, 5, or 7 nodes to maintain quorum). This ensures that the cluster can tolerate node failures without losing data consistency.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

How does etcd ensure data consistency across nodes?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

etcd uses the Raft consensus algorithm to ensure data consistency and fault tolerance. Raft ensures that all changes to the data are agreed upon by the majority of nodes (quorum), providing strong consistency guarantees.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

How can you secure etcd communication?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Secure etcd communication by using TLS certificates for both client-server and peer-to-peer communication. This involves configuring etcd to use the certificates and ensuring all components communicating with etcd are also configured to use these certificates.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

How do you monitor etcd health and performance?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Use monitoring tools like Prometheus to collect metrics from etcd. Key metrics include request latency, data size, leader election times, and health status of nodes. Setting up alerts for anomalies in these metrics helps in proactive management.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

If an etcd node fails, how do you ensure the Kubernetes cluster remains operational?
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The cluster remains operational as long as a majority of etcd nodes (quorum) are available. For a three-node cluster, at least two nodes must be up. Immediate steps include investigating the cause of the failure, replacing the failed node, and restoring it from the latest backup if necessary.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Explain HPA
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In a Horizontal Pod Autoscaler (HPA) within Kubernetes, minReplicas and maxReplicas are settings that define the desired scaling range for a deployment or replica set. They establish boundaries for the HPA controller when it automatically adjusts the number of Pod replicas based on predefined metrics (like CPU or memory usage).

Here's a breakdown of their roles:

minReplicas (minimum replicas):

This setting specifies the absolute minimum number of replicas that the HPA will ensure are always running for the deployment or replica set.
Even if the autoscaling metrics indicate the application doesn't require many resources, the HPA will not scale down below this minimum threshold.
This helps maintain application availability and prevents complete outages during low traffic periods.
maxReplicas (maximum replicas):

This setting defines the upper limit on the number of replicas the HPA can scale up to.
The HPA will not create more replicas than this maximum even if the autoscaling metrics show high resource utilization.
This helps prevent excessive resource consumption and keeps costs under control.
HPA and Autoscaling:

The HPA continuously monitors the target deployment or replica set's resource usage metrics (e.g., CPU, memory). Based on these metrics and predefined scaling thresholds:

If resource usage goes above the target: The HPA scales the deployment up (adds replicas) to handle the increased demand.
If resource usage falls below a specific threshold: The HPA scales the deployment down (removes replicas) to optimize resource utilization.
minReplicas and maxReplicas act as guardrails within this autoscaling process. They ensure the HPA doesn't accidentally scale down to zero replicas (causing an outage) or scale up infinitely (consuming all available resources).

How to Define minReplicas and maxReplicas:

These settings are specified in the HPA YAML configuration file. Here's an example:

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: my-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-deployment
  minReplicas: 2  # Minimum number of replicas (default: 1)
  maxReplicas: 5  # Maximum number of replicas (default: 10)
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80  # Scale up if CPU usage exceeds 80%

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
What are the other ways to forward port
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. NodePort Service
2. LoadBalancer Service
3. Ingress with Different Controllers
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Please help me understand what are the ways we can forward port 443 to port 8080 which running on kubernetes pods , explain with example

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
To forward port 443 (HTTPS) to port 8080 on Kubernetes pods, you can use a Kubernetes Service along with an Ingress resource. Here's a step-by-step explanation and example:

Deployment: Deploy your application on port 8080.
Service: Expose the application within the cluster on port 8080.
Ingress: Use an Ingress resource to handle HTTPS traffic on port 443 and forward it to the Service on port 8080.
TLS Secret: Store your TLS certificate and key in a Kubernetes Secret.
This setup ensures that external traffic to https://my-app.example.com on port 443 is routed to your pods running on port 8080.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: my-app-image
        ports:
        - containerPort: 8080

apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP

Use an Ingress resource to forward traffic from port 443 to the Service on port 8080.
First, make sure you have an Ingress controller installed (e.g., Nginx Ingress Controller).
Create an Ingress resource to handle HTTPS traffic and forward it to the Service.

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - my-app.example.com
    secretName: my-app-tls
  rules:
  - host: my-app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-app-service
            port:
              number: 8080

You need a TLS certificate for HTTPS. Create a Kubernetes Secret to store your TLS certificate and key.

apiVersion: v1
kind: Secret
metadata:
  name: my-app-tls
  namespace: default
data:
  tls.crt: <base64 encoded certificate>
  tls.key: <base64 encoded key>
type: kubernetes.io/tls

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
